# -*- coding: utf-8 -*-
"""00-Keras-Syntax-Grundlagen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13sqnzAmzFFQKB71sRv6Dm3lvAamZ_oxG

<a href='https://akademie.datamics.com/kursliste/'>![title](bg_datamics_top.png)</a>
___
<center><em>© Datamics</em></center>
<center><em>Besuche uns für mehr Informationen auf <a href='https://akademie.datamics.com/kursliste/'>www.akademie.datamics.com</a></em></center>

# Keras Syntax Grundlagen

In TensorFlow 2.0 stellt Keras jetzt die primäre API. Lasst uns ein einfaches Regressionsprojekt betrachten, um die Grundlagen der Keras-Syntax und des hinzufügens von Layern zu erklären.

## Der Datensatz

Zum Erlernen der wichtigsten Grundlagen der Syntax von Keras werden wir einen einfachen Satz falscher Daten verwenden, um dann in späteren Abschnitten mit echten Datensätzen und Featureerkennung zu arbeiten! Lasst uns nun zuerst die Syntax von TensorFlow 2.0 betrachten.

Lasst uns so tun als beinhalte dieser Datensatz Features (Eigenschaften) seltener Edelsteine, mit zwei Features und einem Verkaufspreis. Unser Ziel ist es, den Verkaufspreis eines gerade ausgegrabenen Edelsteines vorherzusagen, um ihn zu einem fairen Preis anbieten zu können.

### Datensatz laden
"""

!sudo apt-get install python3.7

!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1

!sudo update-alternatives --config python3

!sudo apt install python3-pip

!sudo apt-get install python3.7-distutils
print("Python Version:")
!python --version

!pip install pandas
!pip install seaborn
!pip install matplotlib
!pip install scikit-learn
!pip install tensorflow

import pandas as pd

from google.colab import drive
drive.mount('/content/drive/', force_remount=True)
df = pd.read_csv("/content/drive/My Drive/dl-udemy/fake_reg.csv")

# df = pd.read_csv('../DATA/fake_reg.csv')

df.head()

"""### Datensatz erkunden

Lasst uns einen schnellen Blick auf den Datensatz werfen: gibt es eine starke Korrelation zwischen den Features und dem "Preis" der generierten Daten?
"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.pairplot(df)

"""Du kannst die Daten weiter visualisieren, da die Daten aber generiert sind, werden wir uns im Detail erst in späteren Abschnitten des Kurses mit Featureerkennug und Datenexploration beschäftigen!

### Test/Train Aufteilung
"""

from sklearn.model_selection import train_test_split

# Convert Pandas to Numpy for Keras

# Features
X = df[['feature1','feature2']].values

# Label
y = df['price'].values

# Split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)

X_train.shape

X_test.shape

y_train.shape

y_test.shape

"""## Normalisierung und Skalierung der Daten

Wir skalieren die Daten der Features.

[Weshalb das Label nicht skaliert werden muss](https://stats.stackexchange.com/questions/111467/is-it-necessary-to-scale-the-target-value-in-addition-to-scaling-features-for-re)
"""

from sklearn.preprocessing import MinMaxScaler

help(MinMaxScaler)

scaler = MinMaxScaler()

# Notice to prevent data leakage from the test set, we only fit our scaler to the training set

scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

import pickle
scalerfile = 'scaler.sav'
pickle.dump(scaler, open(scalerfile, 'wb'))

"""# TensorFlow 2.0 Syntax


## Import Optionen

Es gibt verschiedene Wege zum Import von Keras in TensorFlow (dies ist stark abhängig von persöhnlichen Vorlieben, bitte verwende die Methode, die dir am besten gefällt). Wir verwenden die Methode aus der **offiziellen TF Dokumentation**.
"""

import tensorflow as tf

from tensorflow.keras import Sequential

help(Sequential)

"""## Ein Modell erzeugen

Die TF2 Keras API bietet zwei Methoden zum erzeugen eines Modells. Übergib entweder eine Liste aller Layers auf einmal oder füge sie nacheinander hinzu.

Wir zeigen beide Methoden (verwende diejenige, die dir besser gefällt).
"""

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Activation

"""### Modell - als eine Liste von Layern"""

model = Sequential([
    Dense(units=2),
    Dense(units=2),
    Dense(units=2)
])

"""### Modell - Layer nach und nach hinzufügen"""

model = Sequential()

model.add(Dense(2))
model.add(Dense(2))
model.add(Dense(2))

"""Lasst uns fortfahren und ein einfaches Modell erst bauen und es dann, nach der Definition des Solvers (Bewertungsfunktion), kompilieren."""

model = Sequential()

model.add(Dense(4,activation='relu'))
model.add(Dense(4,activation='relu'))
model.add(Dense(4,activation='relu'))

# Final output node for prediction
model.add(Dense(1))

model.compile(optimizer='rmsprop',loss='mse')

"""### Optimierer (Optimizer) und loss (Kostenfunktion)

Beachte, welche Art von Problem du lösen möchtest:

    # Für multiklassen-Klassifizierungsprobleme
    model.compile(optimizer='rmsprop',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    # Für binäre Klassifizierungsprobleme
    model.compile(optimizer='rmsprop',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

    # Für ein  Regressionsproblem mit gemittelter quadratischer Abweichung (mean squared error)
    model.compile(optimizer='rmsprop',
                  loss='mse')

# Training

Es folgen einige gebräuchliche Definitionen, die nötig sind zum Verständnis der korrekten Verwendung von Keras:

* Sample (Stichprobe): ein Element eines Datensatzes.
    * Beispiel: ein Bild ist ein Sample eines Bilderkennungsmodells
    * Beispiel: eine Audiodatei ist ein Sample eines Spracherkennungsmodells
* Batch (Stapel): eine Menge von N Samples. Die Samples eines Batch werden unabhängig und prallel prozessiert. Im Training führt ein Batch zu genau einem Update des Modells. Ein Batch bildet die Verteilung der Eingabedaten besser ab als ein einzelnes Bild. Je größer das Batch, desto besser die Abbildung; das Prozessieren des Batches dauert aber entsprechend länger und führt nur zu einem Update. Für die Evaluierung und Vorhersagen ist es empfehlenswert, die maximale vom RAM erlaubte Batchgröße zu wählen (dies ermöglicht eine schnellere Evaluierung und Vorhersage).
* Epoche (Epoch): ein willkürlicher Abschnitt, allgemein definiert als "ein Durchlauf des gesamten Datensatzes", wird verwendet um das Training in verschiedene Phasen aufzuteilen, was hilfreich ist für Logging und periodische Evaluation.
* Wenn validation_data oder validation_split mit den Anpassungsmethoden der Keras-Modelle verwendet werden, wird die Evaluation am Ende jeder Epoche ausgeführt.
* Keras erlaubt es, spezifische  Callbacks zu entwerfen, die am Ende jeder Epoche ausgeführt werden. Beispiele dafür sind Änderungen der Lernrate und Modellcheckpoints (speichern).
"""

model.fit(X_train,y_train,epochs=250)

"""## Evaluation

Lasst uns die Ergebnisse unsereses Trainings- und Testdatensatzes evaluieren. Wir vergleichen deren Ergebnisse und überprüfen, ob ein Overfitting (Überanpassung) vorliegt.
"""

model.history.history

loss = model.history.history['loss']

sns.lineplot(x=range(len(loss)),y=loss)
plt.title("Training Loss per Epoch");

"""### Vergleiche die finale Evaluation (MSE) des Trainings- und Testdatensatzes

Diese sollten hoffentlich ziemlich nahe beieinander liegen.
"""

model.metrics_names

training_score = model.evaluate(X_train,y_train,verbose=0)
test_score = model.evaluate(X_test,y_test,verbose=0)

training_score

test_score

"""### Weitere Evaluationen"""

test_predictions = model.predict(X_test)

test_predictions

pred_df = pd.DataFrame(y_test,columns=['Test Y'])

pred_df

test_predictions = pd.Series(test_predictions.reshape(300,))

test_predictions

pred_df = pd.concat([pred_df,test_predictions],axis=1)

pred_df.columns = ['Test Y','Model Predictions']

pred_df

"""Lasst uns das mit den echten Testlabels vergleichen!"""

sns.scatterplot(x='Test Y',y='Model Predictions',data=pred_df)

pred_df['Error'] = pred_df['Test Y'] - pred_df['Model Predictions']

sns.distplot(pred_df['Error'],bins=50)

from sklearn.metrics import mean_absolute_error,mean_squared_error

mean_absolute_error(pred_df['Test Y'],pred_df['Model Predictions'])

mean_squared_error(pred_df['Test Y'],pred_df['Model Predictions'])

# Essentially the same thing, difference just due to precision
test_score

#RMSE
test_score**0.5

"""# Vorhersage von neuen Daten

Was ist, wenn wir einen neuen Edelstein ausgraben? Welchen Preis sollte er haben? Dies ist der **gleiche** Vorgang wie die Vorhersage neuer Testdaten!
"""

# [[Feature1, Feature2]]
new_gem = [[998,1000]]

# Don't forget to scale!
scaler.transform(new_gem)

new_gem = scaler.transform(new_gem)

model.predict(new_gem)

"""## Speichern und Laden eines Modelles"""

from tensorflow.keras.models import load_model

model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'

later_model = load_model('my_model.h5')

# [[Feature1, Feature2]]
new_gem2 = [[998,1000]]
import pickle
scaler2 = pickle.load(open('scaler.sav', 'rb'))
new_gem2 = scaler2.transform(new_gem2)

later_model.predict(new_gem2)

"""# Gut gemacht!"""